{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xvdKn9F6NFae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31bad6e8-7bcf-4c5d-c97a-7669a7d35d9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive     # Mounting the drive on colab notebook\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/drive/My Drive/Colab Notebooks- 1804/comp1804_coursework_dataset_23-24.csv')\n"
      ],
      "metadata": {
        "id": "YS6V4hIcShyB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcGDTG3KV1qL"
      },
      "source": [
        "# Data Exploration and Assessment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k6025T4RHsrF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f03140c-bbff-4d69-a238-664b9e033db4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 210 duplicates rows.\n"
          ]
        }
      ],
      "source": [
        "# Removing duplicates\n",
        "duplicates_rows = data.duplicated()\n",
        "print(f'There are {duplicates_rows.sum()} duplicates rows.')\n",
        "data = data[~duplicates_rows]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "43EGdvCcW92k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0e590d-2adc-4f83-fa61-2e2049afb14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9137, 8)\n"
          ]
        }
      ],
      "source": [
        "# Shape attribute\n",
        "print(data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "l94W76erBJ9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ee0b50-5f6a-414b-ee2b-ff9b6e10f2e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['par_id', 'paragraph', 'has_entity', 'lexicon_count', 'difficult_words',\n",
            "       'last_editor_gender', 'category', 'text_clarity'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Column attribute\n",
        "print(data.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Yy2duvieDTIh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c7acd12-b576-4ea0-9077-40e6bdca9a6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "par_id                  int64\n",
            "paragraph              object\n",
            "has_entity             object\n",
            "lexicon_count           int64\n",
            "difficult_words       float64\n",
            "last_editor_gender     object\n",
            "category               object\n",
            "text_clarity           object\n",
            "dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Data types of all features\n",
        "print(data.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qJ7XUTQ5EUoH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e180ae-2df4-40e4-a13c-c363012d29d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         par_id                                          paragraph  \\\n",
            "0  428209002237  Ramsay was born in Glasgow on 2 October 1852. ...   \n",
            "1  564218010072  It has been widely estimated for at least the ...   \n",
            "2  291401001672  He went on to win the Royal Medal of the Royal...   \n",
            "3   31548004883  The changes have altered many underlying assum...   \n",
            "4   50634005146  After these novels were published, Disraeli de...   \n",
            "\n",
            "                        has_entity  lexicon_count  difficult_words  \\\n",
            "0   ORG_YES_PRODUCT_NO_PERSON_YES_             49             12.0   \n",
            "1    ORG_YES_PRODUCT_NO_PERSON_NO_            166             47.0   \n",
            "2    ORG_YES_PRODUCT_NO_PERSON_NO_             69             18.0   \n",
            "3    ORG_NO_PRODUCT_YES_PERSON_NO_             76             27.0   \n",
            "4  ORG_YES_PRODUCT_YES_PERSON_YES_            200             47.0   \n",
            "\n",
            "  last_editor_gender                 category      text_clarity  \n",
            "0                man              biographies      clear_enough  \n",
            "1                man  artificial intelligence  not_clear_enough  \n",
            "2         non-binary              biographies      clear_enough  \n",
            "3         non-binary              programming      clear_enough  \n",
            "4                man              biographies  not_clear_enough  \n"
          ]
        }
      ],
      "source": [
        "# Head function\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JceThaJcEtU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65df7ac8-f5cd-46f3-dcd2-74f5b7fc8728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            par_id                                          paragraph  \\\n",
            "1115  452865009663  In 1987, the Biotechnology Laboratory, one of ...   \n",
            "6985  831020011691  In computing, POSIX Threads, commonly known as...   \n",
            "1663    6360003821  It has been argued AI will become so powerful ...   \n",
            "7694  156701007067  Prior to the award of the Nobel Prize in Physi...   \n",
            "1710  792029011421  One argument against dualism is with regard to...   \n",
            "2135   19675000468  Robert Hooke FRS  was an English naturalist, a...   \n",
            "1217  396559009369  If it can be assumed that anything that can sw...   \n",
            "3852   19980004590  Since, however, all output truth values are co...   \n",
            "1602  196502007392  Some critics argue that Pascal's wager, for th...   \n",
            "6940  657766010683   Hunter had taught Edward Jenner who is seen a...   \n",
            "\n",
            "                          has_entity  lexicon_count  difficult_words  \\\n",
            "1115   ORG_YES_PRODUCT_NO_PERSON_NO_             58             17.0   \n",
            "6985   ORG_YES_PRODUCT_NO_PERSON_NO_             86             22.0   \n",
            "1663  ORG_YES_PRODUCT_NO_PERSON_YES_             62             22.0   \n",
            "7694  ORG_YES_PRODUCT_NO_PERSON_YES_            142             36.0   \n",
            "1710    ORG_NO_PRODUCT_NO_PERSON_NO_            111             34.0   \n",
            "2135  ORG_YES_PRODUCT_NO_PERSON_YES_             63             12.0   \n",
            "1217    ORG_NO_PRODUCT_NO_PERSON_NO_             46              4.0   \n",
            "3852    ORG_NO_PRODUCT_NO_PERSON_NO_             21              7.0   \n",
            "1602   ORG_NO_PRODUCT_NO_PERSON_YES_            113             30.0   \n",
            "6940  ORG_YES_PRODUCT_NO_PERSON_YES_            244             68.0   \n",
            "\n",
            "     last_editor_gender                 category text_clarity  \n",
            "1115              woman              biographies          NaN  \n",
            "6985                man              programming          NaN  \n",
            "1663                man  artificial intelligence          NaN  \n",
            "7694              woman              biographies          NaN  \n",
            "1710                man               philosophy          NaN  \n",
            "2135              woman              biographies          NaN  \n",
            "1217                man              programming          NaN  \n",
            "3852  prefer_not_to_say  artificial intelligence          NaN  \n",
            "1602                man               philosophy          NaN  \n",
            "6940                man              biographies          NaN  \n"
          ]
        }
      ],
      "source": [
        "# Sample function\n",
        "print(data.sample(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "FKsoWQg8wCTc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b92de01a-a9a8-48bb-f455-85324178316c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             par_id  lexicon_count  difficult_words\n",
            "count  9.137000e+03    9137.000000      9119.000000\n",
            "mean   3.571775e+11      82.023312        21.532405\n",
            "std    3.220746e+11      63.448698        16.311847\n",
            "min    8.500328e+07       0.000000         0.000000\n",
            "25%    7.019601e+10      33.000000         9.000000\n",
            "50%    2.684380e+11      64.000000        17.000000\n",
            "75%    6.124310e+11     117.000000        30.000000\n",
            "max    1.058779e+12     653.000000       143.000000\n"
          ]
        }
      ],
      "source": [
        "# Statistics about the numerical features\n",
        "print(data.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0C_m0xYyJ8jv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "266f5edf-9b6d-40b7-b04a-fc6366656532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categories and number of occurrences for 'has_entity'\n",
            "ORG_YES_PRODUCT_NO_PERSON_YES_     2999\n",
            "ORG_NO_PRODUCT_NO_PERSON_NO_       2795\n",
            "ORG_YES_PRODUCT_NO_PERSON_NO_      1448\n",
            "ORG_NO_PRODUCT_NO_PERSON_YES_      1349\n",
            "ORG_YES_PRODUCT_YES_PERSON_YES_     292\n",
            "ORG_YES_PRODUCT_YES_PERSON_NO_      124\n",
            "ORG_NO_PRODUCT_YES_PERSON_YES_       64\n",
            "ORG_NO_PRODUCT_YES_PERSON_NO_        42\n",
            "data missing                         24\n",
            "Name: has_entity, dtype: int64\n",
            "\n",
            "Categories and number of occurrences for 'last_editor_gender'\n",
            "man                  6105\n",
            "woman                2414\n",
            "non-binary            353\n",
            "prefer_not_to_say     265\n",
            "Name: last_editor_gender, dtype: int64\n",
            "\n",
            "Categories and number of occurrences for 'category'\n",
            "biographies                             2888\n",
            "philosophy                              2513\n",
            "programming                             1939\n",
            "artificial intelligence                 1527\n",
            "movies about artificial intelligence     162\n",
            "Philosophy                                13\n",
            "Biographies                               13\n",
            "Programming                               10\n",
            "Artificial intelligence                   10\n",
            "Movies about artificial intelligence       1\n",
            "Name: category, dtype: int64\n",
            "\n",
            "Categories and number of occurrences for 'text_clarity'\n",
            "not_clear_enough    41\n",
            "clear_enough        39\n",
            "Name: text_clarity, dtype: int64\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Value count of categorical columns\n",
        "categorical_columns= ['has_entity', 'last_editor_gender', 'category', 'text_clarity']\n",
        "\n",
        "for col in categorical_columns:\n",
        "  print(f\"Categories and number of occurrences for '{col}'\")\n",
        "  print(data[col].value_counts())\n",
        "  print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iywOEso9HCZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f02bdb2b-fa39-4166-fef0-118f14794c1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "par_id                   0\n",
            "paragraph                0\n",
            "has_entity               0\n",
            "lexicon_count            0\n",
            "difficult_words         18\n",
            "last_editor_gender       0\n",
            "category                61\n",
            "text_clarity          9057\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Missing values\n",
        "print(data.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZDOVu5sWta4"
      },
      "source": [
        "# Data Splitting and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JtpUAmfdurlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed269df1-7d68-4562-e0b0-917e273b6f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ORG_YES_PRODUCT_NO_PERSON_YES_     2999\n",
            "ORG_NO_PRODUCT_NO_PERSON_NO_       2795\n",
            "ORG_YES_PRODUCT_NO_PERSON_NO_      1448\n",
            "ORG_NO_PRODUCT_NO_PERSON_YES_      1349\n",
            "ORG_YES_PRODUCT_YES_PERSON_YES_     292\n",
            "ORG_YES_PRODUCT_YES_PERSON_NO_      124\n",
            "ORG_NO_PRODUCT_YES_PERSON_YES_       64\n",
            "ORG_NO_PRODUCT_YES_PERSON_NO_        42\n",
            "NaN                                  24\n",
            "Name: has_entity, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Replacing \"data missing\" with NaN in \"has_entity\" column\n",
        "data.loc[data['has_entity'] == 'data missing', 'has_entity'] = np.nan\n",
        "\n",
        "# Let's verify the changes\n",
        "print(data['has_entity'].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "R32VCLGJwBfN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9bca435-570f-484d-801e-a2d478254806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "par_id                   0\n",
            "paragraph                0\n",
            "has_entity              24\n",
            "lexicon_count            0\n",
            "difficult_words         18\n",
            "last_editor_gender       0\n",
            "category                61\n",
            "text_clarity          9057\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Missing values from all of the features\n",
        "print(data.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4NOF6E0GxiRQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83779d16-e00f-43f0-e4b2-fb3d83f5f3a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values after cleaning:\n",
            " par_id                   0\n",
            "paragraph                0\n",
            "has_entity               0\n",
            "lexicon_count            0\n",
            "difficult_words          0\n",
            "last_editor_gender       0\n",
            "category                 0\n",
            "text_clarity          8972\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Droping rows with missing values in \"has_entity\" and \"category\" columns\n",
        "data.dropna(subset=['has_entity', 'category'], inplace=True)\n",
        "\n",
        "# Imputing missing values in \"difficult_words\" column using the mean stragtegy\n",
        "impt_difficult_words = SimpleImputer(strategy='mean')\n",
        "data['difficult_words'] = impt_difficult_words.fit_transform(data[['difficult_words']])\n",
        "\n",
        "# Verifying missing values after cleaning\n",
        "missing_values_after_cleaning = data.isnull().sum()\n",
        "print(\"Missing values after cleaning:\\n\", missing_values_after_cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "z6kwCzxJz9Uv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf9ff1c8-dbb7-418c-8be3-8edf9bc8ee00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "biographies                             2891\n",
            "philosophy                              2521\n",
            "programming                             1944\n",
            "artificial intelligence                 1534\n",
            "movies about artificial intelligence     162\n",
            "Name: category, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Converting all labels in \"category\" column to lowercase\n",
        "data['category'] = data['category'].str.lower()\n",
        "\n",
        "# Mapping old labels to new labels\n",
        "merge_classes = {\n",
        "    'philosophy': 'philosophy',\n",
        "    'biographies': 'biographies',\n",
        "    'programming': 'programming',\n",
        "    'artificial intelligence': 'artificial intelligence',\n",
        "    'movies about artificial intelligence': 'movies about artificial intelligence'\n",
        "}\n",
        "\n",
        "# Updating the 'category' column\n",
        "data['category'] = data['category'].map(merge_classes)\n",
        "\n",
        "# Updated distribution of classes in \"category\" column\n",
        "print(data['category'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MVj6t4LK32Kl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1dfe1c-c637-4a6f-eadf-e8e2a88239fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "par_id                   0\n",
            "paragraph                0\n",
            "has_entity               0\n",
            "lexicon_count            0\n",
            "difficult_words          0\n",
            "last_editor_gender       0\n",
            "category                 0\n",
            "text_clarity          8972\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Let's recheck the missing values again\n",
        "print(data.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GUAVg2ZqSfsM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c68177b-72f6-426a-bccd-181f809e524f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task 1:\n",
            "Train set shape: (6336, 2) (6336,)\n",
            "Validation set shape: (1358, 2) (1358,)\n",
            "Test set shape: (1358, 2) (1358,)\n",
            "Train dataset length: 56\n",
            "Validation dataset length: 12\n",
            "Test dataset length: 12\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Task 1\n",
        "X_t1 = data[['paragraph', 'has_entity']] # Input Features for Task 1\n",
        "Y_t1 = data['category']  # Target variable for Task 1\n",
        "\n",
        "# Stratified split for Task 1\n",
        "X_train_t1, X_temp_t1, Y_train_t1, Y_temp_t1 = train_test_split(X_t1, Y_t1, test_size=0.3, random_state=42, stratify=Y_t1)\n",
        "X_val_t1, X_test_t1, Y_val_t1, Y_test_t1 = train_test_split(X_temp_t1, Y_temp_t1, test_size=0.5, random_state=42, stratify=Y_temp_t1)\n",
        "\n",
        "print(\"Task 1:\")\n",
        "print(\"Train set shape:\", X_train_t1.shape, Y_train_t1.shape)\n",
        "print(\"Validation set shape:\", X_val_t1.shape, Y_val_t1.shape)\n",
        "print(\"Test set shape:\", X_test_t1.shape, Y_test_t1.shape)\n",
        "\n",
        "# Task 2\n",
        "# Creatingg a subset\n",
        "subset = data.dropna(subset=['text_clarity'])[['paragraph', 'lexicon_count', 'difficult_words', 'text_clarity']]\n",
        "\n",
        "X_t2 = subset[['paragraph', 'lexicon_count', 'difficult_words']] # Input Features for Task 2\n",
        "Y_t2 = subset['text_clarity'] # Target Variable for Task 2\n",
        "\n",
        "# Splitting the subset into train/validation/test datasets\n",
        "X_train_val_2, X_test_2, Y_train_val_2, Y_test_2 = train_test_split(X_t2, Y_t2, test_size=0.15, random_state=42, stratify=Y_t2)\n",
        "X_train_2, X_val_2, Y_train_2, Y_val_2 = train_test_split(X_train_val_2, Y_train_val_2, test_size=0.15/0.85, random_state=42, stratify=Y_train_val_2)\n",
        "\n",
        "# Lengths of train/validation/test datasets\n",
        "print(\"Train dataset length:\", len(X_train_2))\n",
        "print(\"Validation dataset length:\", len(X_val_2))\n",
        "print(\"Test dataset length:\", len(X_test_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2REMBKsfOAx-"
      },
      "source": [
        "# Data Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R0OC8gxWOC3z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ef59343-b3bb-48da-a814-d79b01b85981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Function for text pre-processing\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Applying text preprocessing to \"paragraph\" column for Task 1\n",
        "X_train_t1['paragraph_preprocessed'] = X_train_t1['paragraph'].apply(preprocess_text)\n",
        "X_val_t1['paragraph_preprocessed'] = X_val_t1['paragraph'].apply(preprocess_text)\n",
        "X_test_t1['paragraph_preprocessed'] = X_test_t1['paragraph'].apply(preprocess_text)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1200)\n",
        "\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_t1['paragraph_preprocessed'])\n",
        "X_val_tfidf = tfidf_vectorizer.transform(X_val_t1['paragraph_preprocessed'])\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_t1['paragraph_preprocessed'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DiI4aIzPSljK"
      },
      "outputs": [],
      "source": [
        "# One-hot encoding on \"has_entity\"\n",
        "def one_hot_encoding(text):\n",
        "    entities = text.split('_')\n",
        "    encoded = {'ORG': 0, 'PRODUCT': 0, 'PERSON': 0}\n",
        "    for i in range(0, len(entities) - 1, 2):\n",
        "        entity = entities[i]\n",
        "        status = entities[i + 1] if i + 1 < len(entities) else ''\n",
        "        if status == 'YES':\n",
        "            if 'ORG' in entity:\n",
        "                encoded['ORG'] = 1\n",
        "            elif 'PRODUCT' in entity:\n",
        "                encoded['PRODUCT'] = 1\n",
        "            elif 'PERSON' in entity:\n",
        "                encoded['PERSON'] = 1\n",
        "    return encoded['ORG'], encoded['PRODUCT'], encoded['PERSON']\n",
        "\n",
        "# Applying one-hot encoding to \"has_entity\" column for Task 1\n",
        "X_train_t1['org'], X_train_t1['prod'], X_train_t1['person'] = zip(*X_train_t1['has_entity'].apply(one_hot_encoding))\n",
        "X_val_t1['org'], X_val_t1['prod'], X_val_t1['person'] = zip(*X_val_t1['has_entity'].apply(one_hot_encoding))\n",
        "X_test_t1['org'], X_test_t1['prod'], X_test_t1['person'] = zip(*X_test_t1['has_entity'].apply(one_hot_encoding))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenizing \"paragraph\" text for Task 2\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train_2['paragraph'])\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train_2['paragraph'])\n",
        "X_val_seq = tokenizer.texts_to_sequences(X_val_2['paragraph'])\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test_2['paragraph'])\n",
        "\n",
        "# Padding sequences\n",
        "max_len = 100\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_val_pad = pad_sequences(X_val_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n"
      ],
      "metadata": {
        "id": "c4f5tmuCrqy9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Initializing LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fitting and transforming target labels for training data\n",
        "Y_train_encoded_2 = label_encoder.fit_transform(Y_train_2)\n",
        "Y_val_encoded_2 = label_encoder.transform(Y_val_2)\n",
        "Y_test_encoded_2 = label_encoder.transform(Y_test_2)\n"
      ],
      "metadata": {
        "id": "h8o3tn341Qmu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ptIBNcpTL9_j"
      },
      "outputs": [],
      "source": [
        "# Dropping original columns\n",
        "X_train_t1.drop(columns=['paragraph', 'has_entity'], inplace=True)\n",
        "X_val_t1.drop(columns=['paragraph', 'has_entity'], inplace=True)\n",
        "X_test_t1.drop(columns=['paragraph', 'has_entity'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying column-drop\n",
        "print(X_train_t1.head(3))\n",
        "print(X_val_t1.head(3))\n",
        "print(X_test_t1.head(3))\n"
      ],
      "metadata": {
        "id": "2FAqx2K8R4d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af3d62da-77ff-4640-d8ff-e89416a5302e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 paragraph_preprocessed  org  prod  person\n",
            "6008  although george darwin son famous biologist ch...    1     0       1\n",
            "3051  extreme goal ai research create computer progr...    1     0       0\n",
            "6552  einstein two events taking place points b syst...    1     0       0\n",
            "                                 paragraph_preprocessed  org  prod  person\n",
            "235   thomson first investigated magnetic deflection...    1     0       0\n",
            "575   process breaking concept proposition fact simp...    1     0       1\n",
            "4937  propositions also spoken content beliefs simil...    0     0       0\n",
            "                                 paragraph_preprocessed  org  prod  person\n",
            "8552  socratic seminars based upon interaction peers...    0     0       0\n",
            "2329  discipline history philosophy aims provide sys...    0     0       0\n",
            "4859  building graphical user interfaces way testing...    1     0       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx6Ikz2qjQqy"
      },
      "source": [
        "# Task 1 : Topic Classification"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression model\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Initializing StandardScaler\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "\n",
        "# Scaling TF-IDF transformed features\n",
        "X_train_tfidf_scaled = scaler.fit_transform(X_train_tfidf)\n",
        "X_val_tfidf_scaled = scaler.transform(X_val_tfidf)\n",
        "X_test_tfidf_scaled = scaler.transform(X_test_tfidf)\n",
        "\n",
        "# Combining TF-IDF transformed features with one-hot encoded \"has_entity\" features\n",
        "X_train_combined = hstack([X_train_tfidf_scaled, X_train_t1[['org', 'prod', 'person']].values])\n",
        "X_val_combined = hstack([X_val_tfidf_scaled, X_val_t1[['org', 'prod', 'person']].values])\n",
        "X_test_combined = hstack([X_test_tfidf_scaled, X_test_t1[['org', 'prod', 'person']].values])\n",
        "\n",
        "# Hyperparameters to tune\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [ 0.1, 1, 10],\n",
        "    'solver': [ 'saga'],\n",
        "    'class_weight': [None, 'balanced'],\n",
        "    'max_iter': [1000]\n",
        "}\n",
        "\n",
        "# Initializing GridSearchCV with Logistic Regression model and hyperparameters\n",
        "grid_search = GridSearchCV(LogisticRegression(random_state=42), param_grid, cv=3, scoring='accuracy', verbose=1)\n",
        "\n",
        "# Fitting GridSearchCV to find the best hyperparameters\n",
        "grid_search.fit(X_train_combined, Y_train_t1)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Initializing Logistic Regression model with the best hyperparameters\n",
        "best_logistic_model = LogisticRegression(**best_params, random_state=42)\n",
        "\n",
        "# Training the model on the combined features\n",
        "best_logistic_model.fit(X_train_combined, Y_train_t1)\n",
        "\n",
        "# Predicting on the training set using the trained model\n",
        "train_predictions_lg = best_logistic_model.predict(X_train_combined)\n",
        "\n",
        "# Predicting on the validation set using the best model\n",
        "val_predictions_lg = best_logistic_model.predict(X_val_combined)\n",
        "\n",
        "# Predicting on the test set using the best model\n",
        "test_predictions_lg = best_logistic_model.predict(X_test_combined)\n",
        "\n",
        "# Calculating accuracy on the training set\n",
        "train_accuracy_lg = accuracy_score(Y_train_t1, train_predictions_lg)\n",
        "print(\"Training Accuracy:\", train_accuracy_lg)\n",
        "\n",
        "# Calculating accuracy on the validation set\n",
        "val_accuracy_lg = accuracy_score(Y_val_t1, val_predictions_lg)\n",
        "print(\"Validation Accuracy (Logistic Regression):\", val_accuracy_lg)\n",
        "\n",
        "# Calculating accuracy on the test set\n",
        "test_accuracy_lg = accuracy_score(Y_test_t1, test_predictions_lg)\n",
        "print(\"Test Accuracy (Logistic Regression):\", test_accuracy_lg)\n"
      ],
      "metadata": {
        "id": "2MsPu4zwWk4l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "a702303d-0303-497b-b365-82073cb31c49"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-763d02d31f68>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Fitting GridSearchCV to find the best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_combined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train_t1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Best hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1289\u001b[0m             \u001b[0mn_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n\u001b[0m\u001b[1;32m   1292\u001b[0m             path_func(\n\u001b[1;32m   1293\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1863\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml1_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m             w0, n_iter_i, warm_start_sag = sag_solver(\n\u001b[0m\u001b[1;32m    525\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py\u001b[0m in \u001b[0;36msag_solver\u001b[0;34m(X, y, sample_weight, loss, alpha, beta, max_iter, tol, verbose, random_state, check_input, max_squared_sum, warm_start_mem, is_saga)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0msag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msag64\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msag32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     num_seen, n_iter_ = sag(\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Initializing StandardScaler\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "\n",
        "# Scaling TF-IDF transformed features\n",
        "X_train_tfidf_scaled = scaler.fit_transform(X_train_tfidf)\n",
        "X_val_tfidf_scaled = scaler.transform(X_val_tfidf)\n",
        "X_test_tfidf_scaled = scaler.transform(X_test_tfidf)\n",
        "\n",
        "# Combining TF-IDF transformed features with one-hot encoded \"has_entity\" features\n",
        "X_train_combined = hstack([X_train_tfidf_scaled, X_train_t1[['org', 'prod', 'person']].values])\n",
        "X_val_combined = hstack([X_val_tfidf_scaled, X_val_t1[['org', 'prod', 'person']].values])\n",
        "X_test_combined = hstack([X_test_tfidf_scaled, X_test_t1[['org', 'prod', 'person']].values])\n",
        "\n",
        "\n",
        "# Initializing Random Forest model\n",
        "random_forest_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Initializing GridSearchCV with Random Forest model and hyperparameters\n",
        "grid_search_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fitting GridSearchCV to find the best hyperparameters\n",
        "grid_search_rf.fit(X_train_combined, Y_train_t1)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_params_rf = grid_search_rf.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params_rf)\n",
        "\n",
        "# Initializing Random Forest model with the best hyperparameters\n",
        "best_random_forest_model = RandomForestClassifier(**best_params_rf, random_state=42)\n",
        "\n",
        "# Training the model on the combined features\n",
        "best_random_forest_model.fit(X_train_combined, Y_train_t1)\n",
        "\n",
        "# Predict on the training set using the trained model\n",
        "train_predictions_rf = best_random_forest_model.predict(X_train_combined)\n",
        "\n",
        "# Predicting on the validation set using the best model\n",
        "val_predictions_rf = best_random_forest_model.predict(X_val_combined)\n",
        "\n",
        "# Predicting on the test set using the best model\n",
        "test_predictions_rf = best_random_forest_model.predict(X_test_combined)\n",
        "\n",
        "# Calculating accuracy on the training set\n",
        "train_accuracy_rf = accuracy_score(Y_train_t1, train_predictions_rf)\n",
        "print(\"Training Accuracy:\", train_accuracy_rf)\n",
        "\n",
        "# Calculating accuracy on the validation set\n",
        "val_accuracy_rf = accuracy_score(Y_val_t1, val_predictions_rf)\n",
        "print(\"Validation Accuracy (Random Forest):\", val_accuracy_rf)\n",
        "\n",
        "# Calculating accuracy on the test set\n",
        "test_accuracy_rf = accuracy_score(Y_test_t1, test_predictions_rf)\n",
        "print(\"Test Accuracy (Random Forest):\", test_accuracy_rf)\n"
      ],
      "metadata": {
        "id": "8LMs0vK1Xc81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91df4c4c-3a4e-4a12-a705-30e416d48bf5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
            "Training Accuracy: 0.9977904040404041\n",
            "Validation Accuracy (Random Forest): 0.8232695139911634\n",
            "Test Accuracy (Random Forest): 0.8136966126656848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Support Vector Machine (SVM) model\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initializing StandardScaler\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "\n",
        "# Scaling TF-IDF transformed features\n",
        "X_train_tfidf_scaled = scaler.fit_transform(X_train_tfidf)\n",
        "X_val_tfidf_scaled = scaler.transform(X_val_tfidf)\n",
        "X_test_tfidf_scaled = scaler.transform(X_test_tfidf)\n",
        "\n",
        "# Combining TF-IDF transformed features with one-hot encoded \"has_entity\" features\n",
        "X_train_combined = hstack([X_train_tfidf_scaled, X_train_t1[['org', 'prod', 'person']].values])\n",
        "X_val_combined = hstack([X_val_tfidf_scaled, X_val_t1[['org', 'prod', 'person']].values])\n",
        "X_test_combined = hstack([X_test_tfidf_scaled, X_test_t1[['org', 'prod', 'person']].values])\n",
        "\n",
        "# Hyperparameters to tune\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "# Initializing GridSearchCV with SVM model and hyperparameters\n",
        "svm_grid_search = GridSearchCV(SVC(random_state=42), param_grid, cv=3, scoring='accuracy')\n",
        "\n",
        "# Fitting GridSearchCV to find the best hyperparameters\n",
        "svm_grid_search.fit(X_train_combined, Y_train_t1)\n",
        "\n",
        "# Best hyperparameters\n",
        "best_svm_params = svm_grid_search.best_params_\n",
        "print(\"Best SVM Hyperparameters:\", best_svm_params)\n",
        "\n",
        "# Initializing SVM model with the best hyperparameters\n",
        "best_svm_model = SVC(**best_svm_params, random_state=42)\n",
        "\n",
        "# Training the model on the combined features\n",
        "best_svm_model.fit(X_train_combined, Y_train_t1)\n",
        "\n",
        "# Predicting on the training set using the trained model\n",
        "train_predictions_svm = best_svm_model.predict(X_train_combined)\n",
        "\n",
        "# Predicting on the validation set using the best model\n",
        "val_predictions_svm = best_svm_model.predict(X_val_combined)\n",
        "\n",
        "# Predicting on the test set using the best model\n",
        "test_predictions_svm = best_svm_model.predict(X_test_combined)\n",
        "\n",
        "# Calculating accuracy on the training set\n",
        "train_accuracy_svm = accuracy_score(Y_train_t1, train_predictions_svm)\n",
        "print(\"Training Accuracy:\", train_accuracy_svm)\n",
        "\n",
        "# Calculating accuracy on the validation set\n",
        "val_accuracy_svm = accuracy_score(Y_val_t1, val_predictions_svm)\n",
        "print(\"SVM Validation Accuracy:\", val_accuracy_svm)\n",
        "\n",
        "# Calculating accuracy on the test set\n",
        "test_accuracy_svm = accuracy_score(Y_test_t1, test_predictions_svm)\n",
        "print(\"SVM Test Accuracy:\", test_accuracy_svm)\n"
      ],
      "metadata": {
        "id": "iza_RZykgHz-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cccf5c54-0b3f-44cd-81a0-0b89d8601320"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best SVM Hyperparameters: {'C': 0.1, 'class_weight': 'balanced', 'kernel': 'sigmoid'}\n",
            "Training Accuracy: 0.8969381313131313\n",
            "SVM Validation Accuracy: 0.8755522827687776\n",
            "SVM Test Accuracy: 0.8807069219440353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the final model (SVM)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(Y_test_t1, test_predictions_svm)\n",
        "\n",
        "# Classification Report\n",
        "class_report = classification_report(Y_test_t1, test_predictions_svm)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(Y_test_t1, test_predictions_svm)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)\n",
        "print(\"\\nAccuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "U7aJaST19oxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4c8ec1-c461-4a05-d5aa-f53c904cd24b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[202   4   2  12  10]\n",
            " [ 14 362   4  48   6]\n",
            " [  0   0  25   0   0]\n",
            " [ 16  13   1 341   7]\n",
            " [  7   2   1  15 266]]\n",
            "\n",
            "Classification Report:\n",
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "             artificial intelligence       0.85      0.88      0.86       230\n",
            "                         biographies       0.95      0.83      0.89       434\n",
            "movies about artificial intelligence       0.76      1.00      0.86        25\n",
            "                          philosophy       0.82      0.90      0.86       378\n",
            "                         programming       0.92      0.91      0.92       291\n",
            "\n",
            "                            accuracy                           0.88      1358\n",
            "                           macro avg       0.86      0.91      0.88      1358\n",
            "                        weighted avg       0.89      0.88      0.88      1358\n",
            "\n",
            "\n",
            "Accuracy: 0.8807069219440353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 : Text Clarity Classification Prototype"
      ],
      "metadata": {
        "id": "TgWiq_pxyR7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, Concatenate\n",
        "from keras.losses import binary_crossentropy\n",
        "import numpy as np\n",
        "\n",
        "# Concatenating lexicon count and difficult words columns with padded sequences\n",
        "X_train_pad_with_features = np.concatenate((X_train_pad, X_train_2[['lexicon_count', 'difficult_words']].values), axis=1)\n",
        "X_val_pad_with_features = np.concatenate((X_val_pad, X_val_2[['lexicon_count', 'difficult_words']].values), axis=1)\n",
        "X_test_pad_with_features = np.concatenate((X_test_pad, X_test_2[['lexicon_count', 'difficult_words']].values), axis=1)\n",
        "\n",
        "# Hyperparameters\n",
        "num_words = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "max_len = 100\n",
        "\n",
        "# Main input for the text data\n",
        "main_input = Input(shape=(max_len,), dtype='int32', name='main_input')\n",
        "\n",
        "# Embedding layer for text data\n",
        "embedding_layer = Embedding(input_dim=num_words, output_dim=embedding_dim, input_length=max_len)(main_input)\n",
        "lstm_output = LSTM(128)(embedding_layer)\n",
        "\n",
        "# Ddditional input layers for lexicon_count and difficult_words\n",
        "lexicon_input = Input(shape=(2,), name='lexicon_input')\n",
        "\n",
        "# Concatenating the LSTM output with additional features\n",
        "concatenated = Concatenate()([lstm_output, lexicon_input])\n",
        "\n",
        "# Dense output layer with sigmoid activation for binary classification\n",
        "output = Dense(1, activation='sigmoid')(concatenated)\n",
        "\n",
        "# Defining the model\n",
        "model = Model(inputs=[main_input, lexicon_input], outputs=output)\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(loss=binary_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "history = model.fit([X_train_pad, X_train_2[['lexicon_count', 'difficult_words']].values], Y_train_encoded_2,\n",
        "                    validation_data=([X_val_pad, X_val_2[['lexicon_count', 'difficult_words']].values], Y_val_encoded_2),\n",
        "                    epochs=num_epochs, batch_size=batch_size)\n",
        "\n",
        "# Evaluating the model on training data\n",
        "train_loss, train_accuracy = model.evaluate([X_train_pad, X_train_2[['lexicon_count', 'difficult_words']].values], Y_train_encoded_2)\n",
        "print(\"Training Loss:\", train_loss)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "\n",
        "# Evaluating the model on validation data\n",
        "val_loss, val_accuracy = model.evaluate([X_val_pad, X_val_2[['lexicon_count', 'difficult_words']].values], Y_val_encoded_2)\n",
        "print(\"Validation Loss:\", val_loss)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "\n",
        "# Evaluating the model on test data\n",
        "test_loss, test_accuracy = model.evaluate([X_test_pad, X_test_2[['lexicon_count', 'difficult_words']].values], Y_test_encoded_2)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyFTfr3xC43k",
        "outputId": "df24f498-4586-47ef-bd39-ebdde393d18e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2/2 [==============================] - 4s 848ms/step - loss: 10.2748 - accuracy: 0.4821 - val_loss: 10.8853 - val_accuracy: 0.5000\n",
            "Epoch 2/10\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 10.0952 - accuracy: 0.4821 - val_loss: 10.6888 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "2/2 [==============================] - 0s 176ms/step - loss: 9.8854 - accuracy: 0.4821 - val_loss: 10.4563 - val_accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 9.6237 - accuracy: 0.4821 - val_loss: 10.0765 - val_accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "2/2 [==============================] - 0s 175ms/step - loss: 9.1327 - accuracy: 0.4821 - val_loss: 8.6402 - val_accuracy: 0.5000\n",
            "Epoch 6/10\n",
            "2/2 [==============================] - 0s 204ms/step - loss: 7.6012 - accuracy: 0.4464 - val_loss: 7.2552 - val_accuracy: 0.4167\n",
            "Epoch 7/10\n",
            "2/2 [==============================] - 0s 217ms/step - loss: 6.7188 - accuracy: 0.3214 - val_loss: 6.6630 - val_accuracy: 0.2500\n",
            "Epoch 8/10\n",
            "2/2 [==============================] - 0s 206ms/step - loss: 6.3401 - accuracy: 0.2679 - val_loss: 6.3014 - val_accuracy: 0.2500\n",
            "Epoch 9/10\n",
            "2/2 [==============================] - 0s 209ms/step - loss: 6.1649 - accuracy: 0.1964 - val_loss: 6.0376 - val_accuracy: 0.3333\n",
            "Epoch 10/10\n",
            "2/2 [==============================] - 0s 186ms/step - loss: 5.9657 - accuracy: 0.2143 - val_loss: 5.8593 - val_accuracy: 0.2500\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 5.8762 - accuracy: 0.2143\n",
            "Training Loss: 5.876242637634277\n",
            "Training Accuracy: 0.2142857164144516\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 5.8593 - accuracy: 0.2500\n",
            "Validation Loss: 5.8593316078186035\n",
            "Validation Accuracy: 0.25\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 6.1649 - accuracy: 0.2500\n",
            "Test Loss: 6.164886474609375\n",
            "Test Accuracy: 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Making predictions on test data\n",
        "test_predictions = model.predict([X_val_pad, X_val_2[['lexicon_count', 'difficult_words']].values],)\n",
        "\n",
        "# Converting predictions to binary labels\n",
        "binary_predictions = (test_predictions > 0.5).astype(int)\n",
        "\n",
        "# Getting true labels\n",
        "true_labels = Y_test_encoded_2\n",
        "# Computing confusion matrix\n",
        "conf_matrix = confusion_matrix(true_labels, binary_predictions)\n",
        "\n",
        "# Printing confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Computing classification report\n",
        "class_report = classification_report(true_labels, binary_predictions)\n",
        "\n",
        "# Printing classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLaLyMq4I7hE",
        "outputId": "743afbd5-98c1-4513-f6e0-3a719b6f89d6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 526ms/step\n",
            "Confusion Matrix:\n",
            "[[4 2]\n",
            " [3 3]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.67      0.62         6\n",
            "           1       0.60      0.50      0.55         6\n",
            "\n",
            "    accuracy                           0.58        12\n",
            "   macro avg       0.59      0.58      0.58        12\n",
            "weighted avg       0.59      0.58      0.58        12\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}